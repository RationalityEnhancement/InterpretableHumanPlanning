from interpret_evaluate_pipeline import interpret_evaluate_clusters, \
                                        cluster_importance_weighting
from em_clustering import compute_clusters, save_clusters_data, get_num_participants
from RL2DT.hyperparams import *
import numpy as np
import argparse
import os
import gc


def cluster_human_data(exp_id, num_participants, num_clusters, block, info):
    """
    Make a call to the function performing EM clustering of the state-action
    pairs and save the resulting cluster parameters in the appropriate folder.

    Parameters
    ----------
    exp_id : str
        Name of the experiment for which the clusters of state-action pairs were
        created. The name is used to identify proper folders with the data
    num_participants : int 
        Number of participants whose data was considered
    num_clusters : int
        Number of probabilsitic (EM) clusters to create
    block : str
        Part of the experiment from which the data was taken (for ex. 'test'). 
        Depends on which identifiers have been used in the experiment
    """
    d = f"clustering/em_clustering_results/{exp_id}/{num_clusters}_{num_participants}" + \
        info + ".pkl"
    b = ' '
    print(d)
    print(os.path.isfile(d))
    if not(os.path.isfile(d)):
        print("\n\n\n" + b*25 + "CLUSTERING DATA TO {} CLUSTERS\n".format(num_clusters))
        res = compute_clusters(exp_id, num_participants, num_clusters, block)
        opt_model, labels, ll, pd_env, pd_cl, pd_pt, params, num_p = res
        save_clusters_data(exp_id=exp_id, num_cl=num_clusters, 
                           num_p=num_p, labels=labels, log_lik=ll, 
                           participant_data=[pd_env, pd_cl, pd_pt],
                           smm_params=params, info=info)
    print("\n\n\n" + b*25 + "CLUSTERING DATA TO {}".format(num_clusters) + \
          " CLUSTERS COMPLETED\n")
    
    
def rename_strategy_file(exp_id, best_cl, num_participants, 
                         num_trajs, info):
    """
    Rename a file with info on the computed clusters to label the the clustering
    with the best (lowest) marginal likelihood (or marginal likelihood weighted
    by the size of the clusters with respect to all datapoints).

    Parameters
    ----------
    exp_id : str
        Name of the experiment for which the clusters of state-action pairs were
        created. The name is used to identify proper folders with the data
    best_cl : int
        Number of clusters for which the (weighted) marginal is the highest
    num_participants : int 
        Number of participants whose data was considered
    num_trajs : int
        Number of trajectories generated by the EM clusters used to find
        the descriptions of the clusters
    info : str
        Additional test to add to the name of the file
    """
    cwd = os.getcwd()
    filename = "strategies_" + exp_id + "_" + str(best_cl) + "_" + \
                 str(num_participants) + "_" + str(num_trajs) + info + '.txt'

    newfilename = 'BEST_' + filename
    filepath = cwd + '/interprets_procedure/' + filename
    newfilepath = cwd + '/interprets_procedure/' + newfilename
    os.rename(filepath, newfilepath)
                           
                           
def model_selection(run, exp_id, num_participants, block, max_num_clusters, num_trajs, 
                    max_div, interpret_size, tolerance, num_rollouts, num_samples,
                    num_cands, cand_clusters, elbow_choice, expert_rew, info, log,
                    criterion='marginal', ignore_none=True, begin=1):
    """
    Create max_num_clusters models (descriptions of human planning strategies as
    identified by EM clustering on data from experiment exp_id), and establish 
    the best one. The best model is the one for which its (weighted) marginal 
    likelihood is the highest (so the data from the cluster is most probable 
    under this model)

    Parameters
    ----------
    exp_id : str
        Name of the experiment for which the clusters of state-action pairs will
        be created. The name is used to identify proper folders with the data
    num_participants : int 
        Number of participants whose data to consider
    block : str
        Part of the experiment from which the data is taken (e.g. 'test'). Depends
        on which identifiers have been used in the experiment
    max_num_clusters : int
        Maximum number of probabilsitic (EM) clusters to create. The clusters are
        considered in increasing order from 1,2,...,max_num_clusters
    num_trajectories : int
        Number of trajectories generated by the EM clusters used to find
        the descriptions of the clusters
    (Parameters used by AI-Interpret)
    max_div : float
        See max_divergence in RL2DT.interpret
    interpret_size : int
        See max_depth in RL2DT.interpret
    tolerance : float
        See tolerance in RL2DT.interpret
    num_rollouts : int
        See num_rollouts in RL2DT.interpret
    num_samples : int or float in [0,1]
        See num_samples in RL2DT.interpret
    num_cands : int
        See num_candidates in RL2DT.interpret
    cand_clusters : [ int ]
        See num_candidates in RL2DT.interpret
    elbow_choice : str
        See elbow_choice in RL2DT.interpret
    expert_rew : float
        See mean_rew in RL2DT.interpret
    info : str
        See info in RL2DT.interpret
    log : bool
        Whether to save the data on the computed models in a text file
    criterion : str (optional)
        Which measure to use to choose the best model
    ignore_none : bool (optional)
        Whether to reject models which do not find descriptions for some clusters
    begin : int (optional),
        Which number of clusters to start consider first
    """
    if run != None:
        if info != '':
            info_run = "_" + info + "_run" + str(run)
        else:
            info_run = "_run" + str(run)
    else:
        info_run = "_" + info if info != '' else ''
        
    best_stats = {'marginal': np.inf, 'AIC': np.inf, 'BIC': np.inf}
    best = 0
    for c in range(begin, max_num_clusters+1):
        if num_participants == 0:
            num_participants = get_num_participants(exp_id, c, block)
        cluster_human_data(exp_id, num_participants, c, block, info_run)
        c_weights, _ = cluster_importance_weighting(exp_id, c, num_participants, info_run)
        scores, _ =  interpret_evaluate_clusters(experiment_id=exp_id,
                                                 num_strategies=c,
                                                 num_participants=num_participants,
                                                 num_trajs=num_trajs,
                                                 max_divergence=max_div, 
                                                 size=interpret_size, 
                                                 tolerance=tolerance, 
                                                 num_rollouts=num_rollouts, 
                                                 num_samples=num_samples, 
                                                 num_candidates=num_cands, 
                                                 candidate_clusters=cand_clusters, 
                                                 name_dsl_data=None, 
                                                 demo_path='',
                                                 elbow_method=elbow_choice,
                                                 mean_reward=None,
                                                 expert_reward=expert_rew,
                                                 info=info_run,
                                                 log=log)
        log_marginal_likelihood = scores['log_marginal_likelihood']
        model_stats = {'marginal': -1*log_marginal_likelihood, 
                       'AIC': scores['AIC'], 
                       'BIC': scores['BIC'], 
                       'none': scores['nones']}

        if model_stats[criterion] < best_stats[criterion]:
            if not(ignore_none) or (ignore_none and model_stats['none'] == 0):
               best = c
               best_stats = model_stats.copy()
        gc.collect()
    
    if log:
        if begin != max_num_clusters:
            rename_strategy_file(exp_id, best, num_participants, num_trajs, info_run)
    print('\n\n'+' '*25 +'SELECTED MODEL: {} CLUSTERS, '.format(best) + \
          'LOG MARGINAL LIKELIHOOD: {}, '.format(-1*best_stats['marginal']) + \
          'AIC: {}, BIC: {}'.format(best_stats['AIC'], best_stats['BIC']))
                                                                                                        
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--run', '-r',
                        help='Run number',
                        default=None)
    parser.add_argument('--experiment_id', '-e',
                        type=str,
                        help="Identifier of the experiment to interpret.")
    parser.add_argument('--max_num_strategies', '-s',
                        type=int,
                        help="Maximum number of strategy clusters to consider.")
    parser.add_argument('--num_participants', '-p',
                        type=int,
                        help="Number of participants whose data was taken into account.",
                        default=0)
    parser.add_argument('--num_demos', '-n', 
                        type=int,
                        help="How many sequences of planning operations to use for DNF interpretations.")
    parser.add_argument('--elbow_choice', '-elb',
                        choices={'automatic','manual'},
                        help="Whether to find the candidates for the number of " \
                            +"clusters automatically or use the " \
                            +"candidate_clusters parameter.",
                        default='automatic')
    parser.add_argument('--expert_reward', '-er',
                        type=float,
                        help="Mean reward of the optimal strategy for the problem.")
    parser.add_argument('--candidate_clusters', '-cl',
                        type=int, nargs='+',
                        help="The candidate(s) for the number of clusters in " \
                            +"the data either to consider in their entirety " \
                            +"or to automatically choose from.",
                        default=NUM_CLUSTERS)
    parser.add_argument('--num_candidates', '-nc',
                        type=int,
                        help="The number of candidates for the number of " \
                            +"clusters to consider.",
                        default=NUM_CANDIDATES)
    parser.add_argument('--interpret_size', '-i',
                        type=int,
                        help="Maximum depth of the interpretation tree",
                        default=MAX_DEPTH)
    parser.add_argument('--max_divergence', '-md',
                        type=float,
                        help="How close should the intepration performance in " \
                            +"terms of the reward be to the policy's performance",
                        default=MAX_DIVERGENCE)
    parser.add_argument('--tolerance', '-t',
                        type=float,
                        help="What increase in the percentage of the expected " \
                             "expert reward a formula is achieving is " \
                             "considered significant when comparing a two of them.",
                        default=TOLERANCE)
    parser.add_argument('--num_rollouts', '-rl',
                        type=int,
                        help="How many rolouts to perform to compute mean " \
                            +"return per environment",
                        default=NUM_ROLLOUTS)
    parser.add_argument('--samples', '-sm',
                        type=float,
                        help="How many samples/in what ratio to sample from clusters",
                        default=SPLIT)
    parser.add_argument('--info', '-f',
                        type=str,
                        help="What to add to the name of all the output files",
                        default='')
    parser.add_argument('--log', '-l',
                        type=bool,
                        help="Whether to log the interpretation and their evaluations",
                        default=True)
    parser.add_argument('--criterion', '-cr',
                        type=str,
                        choices={'marginal', 'AIC', 'BIC'},
                        help="Based on what measure to choose between the models",
                        default='marginal')
    parser.add_argument('--block', '-b',
                        help="Which block of the experiment to use. Default is all",
                        default=None)
    parser.add_argument('--ignore_none', '-g',
                        help="Ignore models that find no descriptions for some clusters",
                        default=True)
    parser.add_argument('--begin', '-bg',
                        help="Which mimimum number of clusters to consider",
                        type=int,
                        default=1)
                        
    args = parser.parse_args()
    
    model_selection(run=args.run,
                    exp_id=args.experiment_id,
                    num_participants=args.num_participants,
                    block=args.block,
                    max_num_clusters=args.max_num_strategies,
                    num_trajs=args.num_demos,
                    max_div=args.max_divergence, 
                    interpret_size=args.interpret_size, 
                    tolerance=args.tolerance, 
                    num_rollouts=args.num_rollouts, 
                    num_samples=args.samples, 
                    num_cands=args.num_candidates, 
                    cand_clusters=args.candidate_clusters, 
                    elbow_choice=args.elbow_choice,
                    expert_rew=args.expert_reward,
                    info=args.info,
                    log=args.log,
                    criterion=args.criterion,
                    ignore_none=args.ignore_none,
                    begin=args.begin)
                    
